{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cJ4kOA1RoQtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bc0fab1-ba77-4f9f-8e8e-a46a176aa5b7"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QBRDxiBOua6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vvIGw4Gur-NG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Node** **information**"
      ]
    },
    {
      "metadata": {
        "id": "zrz3sfyJpbj5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columes = [\"id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVMatx55sMvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"node_information.csv\",names=columes )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcuQE02jsfbs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train** **set**"
      ]
    },
    {
      "metadata": {
        "id": "RkHWe5HysdrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_set = np.loadtxt(\"training_set.txt\").astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wA53gL6ssoVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = data_set[:, :2]\n",
        "label = data_set[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2y6XTBiHsu3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**K**-**fold** **split**"
      ]
    },
    {
      "metadata": {
        "id": "qx3UpO8fsso3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d8f8f7a0-8d1e-4efc-fcca-815e3f216c48"
      },
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "train_all = []\n",
        "evaluate_all = []\n",
        "skf = StratifiedKFold(n_splits=k, random_state=1234, shuffle=True)\n",
        "for train_index, evaluate_index in skf.split(data, label):\n",
        "    train_all.append(train_index)\n",
        "    evaluate_all.append(evaluate_index)\n",
        "    print(train_index.shape,evaluate_index.shape) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(492409,) (123103,)\n",
            "(492409,) (123103,)\n",
            "(492410,) (123102,)\n",
            "(492410,) (123102,)\n",
            "(492410,) (123102,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mEqMlTbBs19E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(fold_index, max_sentence_len):\n",
        "    tokenizer = Tokenizer(num_words=3000)  \n",
        "    texts = list(df[\"abstract\"].values)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    \n",
        "    train_index = train_all[fold_index - 1]\n",
        "    evaluate_index = evaluate_all[fold_index - 1]\n",
        "    \n",
        "    # group train data\n",
        "    train_label = label[train_index]\n",
        "    \n",
        "    train_input = data[train_index]\n",
        "    paper_id1 = train_input[:, 0]\n",
        "    paper_id2 = train_input[:, 1]\n",
        "    train_input1 = [df.loc[df['id']==idx1]['abstract'].values[0] for idx1 in paper_id1]\n",
        "    train_input2 = [df.loc[df['id']==idx2]['abstract'].values[0] for idx2 in paper_id2]\n",
        "    # convert texts to numbers\n",
        "    x_train_seq1 = tokenizer.texts_to_sequences(train_input1)\n",
        "    x_train_seq2 = tokenizer.texts_to_sequences(train_input2)\n",
        "\n",
        "    x_train1 = sequence.pad_sequences(x_train_seq1, maxlen=max_sentence_len)\n",
        "    x_train2 = sequence.pad_sequences(x_train_seq2, maxlen=max_sentence_len)\n",
        "    \n",
        "    # group evaluate data\n",
        "    eval_label = label[evaluate_index]\n",
        "    \n",
        "    eval_input = data[evaluate_index]\n",
        "    paper_id1_ = eval_input[:, 0]\n",
        "    paper_id2_ = eval_input[:, 1]\n",
        "    eval_input1 = [df.loc[df['id']==idx1]['abstract'].values[0] for idx1 in paper_id1_]\n",
        "    eval_input2 = [df.loc[df['id']==idx2]['abstract'].values[0] for idx2 in paper_id2_]\n",
        "    # convert texts to numbers\n",
        "    x_eval_seq1 = tokenizer.texts_to_sequences(eval_input1)\n",
        "    x_eval_seq2 = tokenizer.texts_to_sequences(eval_input2)\n",
        "\n",
        "    x_eval1 = sequence.pad_sequences(x_eval_seq1, maxlen=max_sentence_len)\n",
        "    x_eval2 = sequence.pad_sequences(x_eval_seq2, maxlen=max_sentence_len)\n",
        "    \n",
        "\n",
        "    return x_train1, x_train2, train_label, x_eval1, x_eval2, eval_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I4XaCj8s7-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create Model**"
      ]
    },
    {
      "metadata": {
        "id": "hIhlo_Shs5xQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(maxlen=150, max_features=3000, embed_size=32):\n",
        "    # Inputs\n",
        "    input1 = Input(shape=[maxlen], name='x_seq1')\n",
        "    input2 = Input(shape=[maxlen], name='x_seq2')\n",
        "\n",
        "    # Embeddings layers\n",
        "    shared_embedding_layer = Embedding(max_features, embed_size)\n",
        "    \n",
        "    # two inputs share embedding layer\n",
        "    encoded1 = shared_embedding_layer(input1)\n",
        "    encoded2 = shared_embedding_layer(input2)\n",
        "\n",
        "    # conv layers\n",
        "    convs1 = []\n",
        "    convs2 = []\n",
        "    filter_sizes = [2, 3, 4, 5]\n",
        "    for fsz in filter_sizes:\n",
        "        conv_layer = Conv1D(filters=64, kernel_size=fsz, activation='relu')\n",
        "        pool_layer = MaxPooling1D(maxlen - fsz + 1)\n",
        "        flatten_layer = Flatten()\n",
        "        # two encoded vectors share the cnn weights\n",
        "        l_conv1 = flatten_layer(pool_layer(conv_layer(encoded1)))\n",
        "        l_conv2 = flatten_layer(pool_layer(conv_layer(encoded2)))\n",
        "        \n",
        "        convs1.append(l_conv1)\n",
        "        convs2.append(l_conv2)\n",
        "        \n",
        "    merge1 = concatenate(convs1, axis=1)\n",
        "    merge2 = concatenate(convs2, axis=1)\n",
        "    \n",
        "    # merge two branches\n",
        "    merge = concatenate([merge1, merge2], axis=-1)\n",
        "\n",
        "    # out = Dropout(0.5)(merge)\n",
        "    hidden1 = Dense(64, activation='relu')(merge)\n",
        "    hidden2 = Dense(32, activation='relu')(hidden1)\n",
        "\n",
        "    output = Dense(units=1, activation='sigmoid')(hidden2)\n",
        "    model = Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gDAU8TSjtF1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ]
    },
    {
      "metadata": {
        "id": "MEfQ3rIZtDAP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train1, x_train2, train_label, x_eval1, x_eval2, eval_label = get_data(1, 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5jeqJ02yXf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_history(history,metric_name):\n",
        "    fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\n",
        "    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
        "    ax_loss.legend()\n",
        "    ax_score.plot(history.epoch, history.history[metric_name], label=\"Train score\")\n",
        "    ax_score.plot(history.epoch, history.history[\"val_\" + metric_name], label=\"Validation score\")\n",
        "    ax_score.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gc1QUVyuwPRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def f1(label, pred):\n",
        "    return tf.py_func(f1_score, [label, (pred>0.5)], tf.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uxaGecrtL_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oWV4XQF5tWqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1905
        },
        "outputId": "e4e90e04-ccee-4c21-c24e-6ab6346c3ef6"
      },
      "cell_type": "code",
      "source": [
        "save_model_name = \"model.ckpt\"\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(save_model_name, monitor='val_acc', \n",
        "                               mode='max', save_best_only=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max',\n",
        "                              factor=0.5, patience=10, min_lr=0.0000001, verbose=1)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "history = model.fit(x=[x_train1, x_train2],\n",
        "          y=train_label,\n",
        "          validation_data=[[x_eval1, x_eval2], eval_label],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[ model_checkpoint,reduce_lr],\n",
        "          verbose=2,\n",
        "          shuffle=True)\n",
        "\n",
        "plot_history(history,'accuracy')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 492409 samples, validate on 123103 samples\n",
            "Epoch 1/50\n",
            " - 301s - loss: 0.4111 - acc: 0.8058 - val_loss: 0.3332 - val_acc: 0.8564\n",
            "Epoch 2/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_accuracy available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_accuracy` which is not available. Available metrics are: val_loss,val_acc,loss,acc,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " - 304s - loss: 0.3007 - acc: 0.8738 - val_loss: 0.2942 - val_acc: 0.8789\n",
            "Epoch 3/50\n",
            " - 304s - loss: 0.2568 - acc: 0.8960 - val_loss: 0.2687 - val_acc: 0.8907\n",
            "Epoch 4/50\n",
            " - 300s - loss: 0.2298 - acc: 0.9088 - val_loss: 0.2617 - val_acc: 0.8951\n",
            "Epoch 5/50\n",
            " - 302s - loss: 0.2088 - acc: 0.9188 - val_loss: 0.2455 - val_acc: 0.9033\n",
            "Epoch 6/50\n",
            " - 305s - loss: 0.1925 - acc: 0.9258 - val_loss: 0.2405 - val_acc: 0.9069\n",
            "Epoch 7/50\n",
            " - 302s - loss: 0.1800 - acc: 0.9315 - val_loss: 0.2445 - val_acc: 0.9064\n",
            "Epoch 8/50\n",
            " - 300s - loss: 0.1690 - acc: 0.9363 - val_loss: 0.2339 - val_acc: 0.9102\n",
            "Epoch 9/50\n",
            " - 304s - loss: 0.1583 - acc: 0.9411 - val_loss: 0.2498 - val_acc: 0.9067\n",
            "Epoch 10/50\n",
            " - 305s - loss: 0.1499 - acc: 0.9445 - val_loss: 0.2439 - val_acc: 0.9085\n",
            "Epoch 11/50\n",
            " - 300s - loss: 0.1422 - acc: 0.9473 - val_loss: 0.2326 - val_acc: 0.9138\n",
            "Epoch 12/50\n",
            " - 304s - loss: 0.1350 - acc: 0.9502 - val_loss: 0.2380 - val_acc: 0.9119\n",
            "Epoch 13/50\n",
            " - 301s - loss: 0.1285 - acc: 0.9526 - val_loss: 0.2375 - val_acc: 0.9161\n",
            "Epoch 14/50\n",
            " - 305s - loss: 0.1228 - acc: 0.9549 - val_loss: 0.2355 - val_acc: 0.9175\n",
            "Epoch 15/50\n",
            " - 299s - loss: 0.1172 - acc: 0.9571 - val_loss: 0.2756 - val_acc: 0.9028\n",
            "Epoch 16/50\n",
            " - 306s - loss: 0.1125 - acc: 0.9587 - val_loss: 0.2402 - val_acc: 0.9164\n",
            "Epoch 17/50\n",
            " - 302s - loss: 0.1076 - acc: 0.9606 - val_loss: 0.2425 - val_acc: 0.9163\n",
            "Epoch 18/50\n",
            " - 303s - loss: 0.1034 - acc: 0.9626 - val_loss: 0.2539 - val_acc: 0.9165\n",
            "Epoch 19/50\n",
            " - 303s - loss: 0.0993 - acc: 0.9642 - val_loss: 0.2513 - val_acc: 0.9154\n",
            "Epoch 20/50\n",
            " - 302s - loss: 0.0961 - acc: 0.9650 - val_loss: 0.2477 - val_acc: 0.9186\n",
            "Epoch 21/50\n",
            " - 301s - loss: 0.0925 - acc: 0.9666 - val_loss: 0.2570 - val_acc: 0.9175\n",
            "Epoch 22/50\n",
            " - 306s - loss: 0.0891 - acc: 0.9678 - val_loss: 0.2641 - val_acc: 0.9172\n",
            "Epoch 23/50\n",
            " - 305s - loss: 0.0861 - acc: 0.9688 - val_loss: 0.2644 - val_acc: 0.9174\n",
            "Epoch 24/50\n",
            " - 299s - loss: 0.0834 - acc: 0.9697 - val_loss: 0.2778 - val_acc: 0.9150\n",
            "Epoch 25/50\n",
            " - 307s - loss: 0.0810 - acc: 0.9704 - val_loss: 0.2780 - val_acc: 0.9169\n",
            "Epoch 26/50\n",
            " - 303s - loss: 0.0781 - acc: 0.9718 - val_loss: 0.2891 - val_acc: 0.9167\n",
            "Epoch 27/50\n",
            " - 300s - loss: 0.0763 - acc: 0.9725 - val_loss: 0.2748 - val_acc: 0.9176\n",
            "Epoch 28/50\n",
            " - 304s - loss: 0.0743 - acc: 0.9731 - val_loss: 0.2860 - val_acc: 0.9148\n",
            "Epoch 29/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a36fb65ef1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m           shuffle=True)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cFAysAiGyiZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd950051-ba80-4c07-d8c6-f8f182900932"
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate([x_eval1[:1000], x_eval2[:1000]], eval_label[:1000])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 1s 971us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jW7yx_7xVAyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11227fee-21aa-4d62-e557-41b7809361fa"
      },
      "cell_type": "code",
      "source": [
        "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_loss: 0.290044, accuracy: 0.913000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DkoTViQsVDzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55cd702f-e775-4fea-c561-bb6ca8aabf0f"
      },
      "cell_type": "code",
      "source": [
        "pred = model.predict(x=[x_eval1, x_eval2])\n",
        "f1 = f1_score(eval_label, (pred>0.5).astype(np.int8))\n",
        "print(f1)                    \n",
        "                      "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.920840283099671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IuJsY3ADbfI5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**load test data and create submission file**"
      ]
    },
    {
      "metadata": {
        "id": "8ydGXUWSVwOn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_set = np.loadtxt(\"testing_set.txt\").astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ntJrU1Kubh1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=3000)  \n",
        "texts = list(df[\"abstract\"].values)\n",
        "tokenizer.fit_on_texts(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnqGKGgQb_UP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_paper_id1 = test_set[:, 0]\n",
        "test_paper_id2 = test_set[:, 1]\n",
        "test_input1 = [df.loc[df['id']==idx1]['abstract'].values[0] for idx1 in test_paper_id1]\n",
        "test_input2 = [df.loc[df['id']==idx2]['abstract'].values[0] for idx2 in test_paper_id2]\n",
        "# convert texts to numbers\n",
        "x_test_seq1 = tokenizer.texts_to_sequences(test_input1)\n",
        "x_test_seq2 = tokenizer.texts_to_sequences(test_input2)\n",
        "\n",
        "x_test1 = sequence.pad_sequences(x_test_seq1, maxlen=150)\n",
        "x_test2 = sequence.pad_sequences(x_test_seq2, maxlen=150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HafN0b_8cEi3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction = model.predict(x=[x_test1, x_test2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nerpncd5cW_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction = (prediction>0.5).astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHaU8J5oeZu6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_dict = {idx: prediction[idx] for idx in range(len(prediction))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nA26ngI8ecw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
        "submission.index.names = ['id']\n",
        "submission.columns = ['category']\n",
        "submission.to_csv(\"submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UN35-0V5ejwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}